{{ if .Values.consensusnode.full.enabled }}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "taraxa-consensus-node.fullname" . }}-full
  labels:
    helm.sh/chart: {{ include "taraxa-node.chart" . }}
    app.kubernetes.io/name: {{ .Release.Name }}-consensus-node
    app.kubernetes.io/type: fullnode
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.consensusnode.full.replicaCount }}
  serviceName: {{ include "taraxa-consensus-node.fullname" . }}-full
  # to launch or terminate all Pods in parallel.
  # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#parallel-pod-management
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      partition: a
      app.kubernetes.io/type: fullnode
      app.kubernetes.io/name: {{ .Release.Name }}-consensus-node
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      name: consensus-node
      labels:
        partition: a
        app.kubernetes.io/type: fullnode
        app.kubernetes.io/name: {{ .Release.Name }}-consensus-node
        app.kubernetes.io/instance: {{ .Release.Name }}
      annotations:
        kubernetes.io/change-cause: "Configuration through configmaps."
    spec:
      initContainers:
        {{ if .Values.explorer.enabled }}
        - name: wait-for-explorer
          image: dwdraju/alpine-curl-jq:latest
          command: ["/bin/entrypoint.sh"]
          volumeMounts:
            - name: explorer-check
              mountPath: /bin/entrypoint.sh
              readOnly: true
              subPath: entrypoint.sh
        {{- end }}
        - name: config-adapter
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          envFrom:
            - secretRef:
                name: {{ .Release.Name }}
          env:
          - name: HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          command: ["/bin/entrypoint.sh"]
          volumeMounts:
            - name: initconfig
              mountPath: /bin/entrypoint.sh
              readOnly: true
              subPath: entrypoint.sh
            - name: initconfig
              mountPath: /bin/genconfig.py
              readOnly: true
              subPath: genconfig.py
            - name: data
              mountPath: /root/.taraxa
      containers:
        {{- if .Values.slack.enabled }}
        - name: status
          image: "python:3.8"
          imagePullPolicy: IfNotPresent
          env:
          - name: SLACK_TOKEN
            valueFrom:
              secretKeyRef:
                name: {{ .Release.Name }}
                key: SLACK_TOKEN
          - name: SLACK_CHANNEL
            value: {{ .Values.slack.channel }}
          - name: K8S_CLUSTER
            value: {{ .Values.slack.k8s_cluster }}
          command: ["/bin/bash", "-c", "--"]
          args: [ "pip install -r /app/requirements.txt && python /app/status.py" ]
          volumeMounts:
            - name: status-requirements
              mountPath: /app/requirements.txt
              readOnly: true
              subPath: requirements.txt
            - name: status-script
              mountPath: /app/status.py
              readOnly: true
              subPath: status.py
        {{- end }}
        - name: consensus-node
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          args:
            {{- toYaml .Values.consensusnode.full.args | nindent 12 }}
          env:
            - name: DEBUG
              value: "{{ .Values.consensusnode.full.debug }}"
            - name: HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          {{- if not .Values.consensusnode.full.probes.enabled }}
            - name: TARAXA_SLEEP_DIAGNOSE
              value: "true"
          {{- end }}
          ports:
            {{- toYaml .Values.consensusnode.full.ports | nindent 12 }}
          {{- if .Values.consensusnode.full.probes.enabled }}
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - "ps -A | grep taraxad"
            initialDelaySeconds: 10
            periodSeconds: 5
          readinessProbe:
            exec:
              command:
              - curl
              - -X
              - POST
              - -H
              - "'Content-Type: application/json'"
              - -d
              - "'{\"jsonrpc\":\"2.0\",\"method\":\"taraxa_protocolVersion\",\"params\": [],\"id\":1}'"
              - http://127.0.0.1:7777
            initialDelaySeconds: 10
            periodSeconds: 5
          {{- end }}
          resources:
            {{- toYaml .Values.consensusnode.full.resources | nindent 12 }}
          volumeMounts:
            - name: data
              mountPath: /root/.taraxa
          securityContext:
            capabilities:
              add:
              - SYS_PTRACE
      {{- with .Values.consensusnode.full.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
    {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
    {{- end }}
      volumes:
        - name: initconfig
          configMap:
            defaultMode: 0700
            name: {{ .Release.Name }}-consensus-node-full-init-script
        {{ if .Values.explorer.enabled }}
        - name: explorer-check
          configMap:
            defaultMode: 0700
            name: {{ include "taraxa-node.fullname" . }}-explorer-check
        {{- end }}
        {{- if .Values.slack.enabled }}
        - name: status-requirements
          configMap:
            defaultMode: 0700
            name: {{ .Release.Name }}-node-status-script
        - name: status-script
          configMap:
            defaultMode: 0700
            name: {{ .Release.Name }}-node-status-script
        {{- end }}
       {{- if not .Values.consensusnode.full.persistence.enabled }}
        - name: data
          emptyDir: {}
       {{- end }}
  {{- if .Values.consensusnode.full.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: data
      annotations:
        {{- if .Values.consensusnode.full.persistence.annotations}}
        {{- toYaml .Values.consensusnode.full.persistence.annotations | nindent 4 }}
        {{- end }}
    spec:
      accessModes:
        - {{ .Values.consensusnode.full.persistence.accessMode | quote }}
    {{- if .Values.consensusnode.full.persistence.storageClass }}
    {{- if (eq "-" .Values.consensusnode.full.persistence.storageClass) }}
      storageClassName: ""
    {{- else }}
      storageClassName: "{{ .Values.consensusnode.full.persistence.storageClass }}"
    {{- end }}
    {{- end }}
      resources:
        requests:
          storage: "{{ .Values.consensusnode.full.persistence.size }}"
  {{- end }}
{{- end }}

{{ if .Values.consensusnode.lite.enabled }}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "taraxa-consensus-node.fullname" . }}-lite
  labels:
    helm.sh/chart: {{ include "taraxa-node.chart" . }}
    app.kubernetes.io/name: {{ .Release.Name }}-consensus-node
    app.kubernetes.io/type: litenode
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.consensusnode.lite.replicaCount }}
  serviceName: {{ include "taraxa-consensus-node.fullname" . }}-lite
  # to launch or terminate all Pods in parallel.
  # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#parallel-pod-management
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      partition: a
      app.kubernetes.io/type: litenode
      app.kubernetes.io/name: {{ .Release.Name }}-consensus-node
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      name: consensus-node
      labels:
        partition: a
        app.kubernetes.io/type: litenode
        app.kubernetes.io/name: {{ .Release.Name }}-consensus-node
        app.kubernetes.io/instance: {{ .Release.Name }}
      annotations:
        kubernetes.io/change-cause: "Configuration through configmaps."
    spec:
      initContainers:
        {{ if .Values.explorer.enabled }}
        - name: wait-for-explorer
          image: dwdraju/alpine-curl-jq:latest
          command: ["/bin/entrypoint.sh"]
          volumeMounts:
            - name: explorer-check
              mountPath: /bin/entrypoint.sh
              readOnly: true
              subPath: entrypoint.sh
        {{- end }}
        - name: config-adapter
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          envFrom:
            - secretRef:
                name: {{ .Release.Name }}
          env:
          - name: HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          command: ["/bin/entrypoint.sh"]
          volumeMounts:
            - name: initconfig
              mountPath: /bin/entrypoint.sh
              readOnly: true
              subPath: entrypoint.sh
            - name: initconfig
              mountPath: /bin/genconfig.py
              readOnly: true
              subPath: genconfig.py
            - name: data
              mountPath: /root/.taraxa
      containers:
        {{- if .Values.slack.enabled }}
        - name: status
          image: "python:3.8"
          imagePullPolicy: IfNotPresent
          env:
          - name: SLACK_TOKEN
            valueFrom:
              secretKeyRef:
                name: {{ .Release.Name }}
                key: SLACK_TOKEN
          - name: SLACK_CHANNEL
            value: {{ .Values.slack.channel }}
          - name: K8S_CLUSTER
            value: {{ .Values.slack.k8s_cluster }}
          command: ["/bin/bash", "-c", "--"]
          args: [ "pip install -r /app/requirements.txt && python /app/status.py" ]
          volumeMounts:
            - name: status-requirements
              mountPath: /app/requirements.txt
              readOnly: true
              subPath: requirements.txt
            - name: status-script
              mountPath: /app/status.py
              readOnly: true
              subPath: status.py
        {{- end }}
        - name: consensus-node
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          args:
            {{- toYaml .Values.consensusnode.lite.args | nindent 12 }}
          env:
            - name: DEBUG
              value: "{{ .Values.consensusnode.lite.debug }}"
            - name: HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          {{- if not .Values.consensusnode.lite.probes.enabled }}
            - name: TARAXA_SLEEP_DIAGNOSE
              value: "true"
          {{- end }}
          ports:
            {{- toYaml .Values.consensusnode.lite.ports | nindent 12 }}
          {{- if .Values.consensusnode.lite.probes.enabled }}
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - "ps -A | grep taraxad"
            initialDelaySeconds: 10
            periodSeconds: 5
          readinessProbe:
            exec:
              command:
              - curl
              - -X
              - POST
              - -H
              - "'Content-Type: application/json'"
              - -d
              - "'{\"jsonrpc\":\"2.0\",\"method\":\"taraxa_protocolVersion\",\"params\": [],\"id\":1}'"
              - http://127.0.0.1:7777
            initialDelaySeconds: 10
            periodSeconds: 5
          {{- end }}
          resources:
            {{- toYaml .Values.consensusnode.lite.resources | nindent 12 }}
          volumeMounts:
            - name: data
              mountPath: /root/.taraxa
          securityContext:
            capabilities:
              add:
              - SYS_PTRACE
      {{- with .Values.consensusnode.lite.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
    {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
    {{- end }}
      volumes:
        - name: initconfig
          configMap:
            defaultMode: 0700
            name: {{ .Release.Name }}-consensus-node-lite-init-script
        {{ if .Values.explorer.enabled }}
        - name: explorer-check
          configMap:
            defaultMode: 0700
            name: {{ include "taraxa-node.fullname" . }}-explorer-check
        {{- end }}
        {{- if .Values.slack.enabled }}
        - name: status-requirements
          configMap:
            defaultMode: 0700
            name: {{ .Release.Name }}-node-status-script
        - name: status-script
          configMap:
            defaultMode: 0700
            name: {{ .Release.Name }}-node-status-script
        {{- end }}
       {{- if not .Values.consensusnode.lite.persistence.enabled }}
        - name: data
          emptyDir: {}
       {{- end }}
  {{- if .Values.consensusnode.lite.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: data
      annotations:
        {{- if .Values.consensusnode.lite.persistence.annotations}}
        {{- toYaml .Values.consensusnode.lite.persistence.annotations | nindent 4 }}
        {{- end }}
    spec:
      accessModes:
        - {{ .Values.consensusnode.lite.persistence.accessMode | quote }}
    {{- if .Values.consensusnode.lite.persistence.storageClass }}
    {{- if (eq "-" .Values.consensusnode.lite.persistence.storageClass) }}
      storageClassName: ""
    {{- else }}
      storageClassName: "{{ .Values.consensusnode.lite.persistence.storageClass }}"
    {{- end }}
    {{- end }}
      resources:
        requests:
          storage: "{{ .Values.consensusnode.lite.persistence.size }}"
  {{- end }}
{{- end }}
